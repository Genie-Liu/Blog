# CTR预估模型之DeepFM

2016年google推出了wide&deep模型之后，基本上就成为了各大推荐系统的标配。该模型既能学习到低阶的记忆性，又能学习到高阶部分的泛化性，所以整体效果来说是比较好的。

但是它有个麻烦的地方，那就是wide部分需要较多的特征工程工作。这一点对于人员紧张的小厂来说还是不太方便。而FM具有自动学习交叉特征的能力，同时其使用的隐变量也可以跟Deep部分一起共享。所以也就有了DeepFM这个模型，用FM来代替wide部分。

## FM简单介绍

FM模型把稀疏特征映射为K维的隐变量，并且通过隐变量之间的点积来作为两个特征交叉的权重值.当然FM也可以做高阶的特征交叉，但是绝大部分时候我们还是只用二阶部分，更高阶的部分还是交给深度网络吧。

整体来看FM解决了以下问题：

1. 降低交叉特征所需要训练的权重数量。使用特征隐变量内积作为两个交叉特征的权重，隐变量的总参数为$K*N$

2. 降低交叉特征的计算复杂度, 从原来的$O(kn^2)$降低为$O(kn)$. 当然对于稀疏场景来说N不算太大. 这一操作使得在训练时候，SGD的运算复杂度也为O(K*N).

   <p><img src="./src/fm_formular_reduce_complexity.png" weidth=200></p>
   <p><img src="./src/fm_sgd.png" weidth=200></p>
   
3. 解决从未出现过的交叉特征问题。论文中举例了电影推荐的场景，很多时候未出现的交叉特征并不代表他们没有相关性。

对比FM和Wide&Deep的Wide部分：

优点：

* 无需特征工程，这一点对于搭建端到端模型来说比较重要。
* 不要求交叉特征共同在样本出现过，这在稀疏场景下比较重要。

缺点：

* 可解释性不如wide
* 计算高阶交叉时需要对全员做交叉，wide则可以通过特征工程选择部分交叉特征

## DeepFM

尽管FM做了特征交叉，但是整体而言还是属于比较低阶的特征，模型学习到的也是特征共现性，即记忆特征。要想提高模型的泛化能力，还是需要加入Deep部分。




尽管FM

参考资料：
1. [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
2. [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/abs/1703.04247)
